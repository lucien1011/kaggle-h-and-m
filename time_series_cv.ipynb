{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7c32c29-8adf-4760-bcc0-dfcab4dd3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3049de8b-4917-4d15-b56c-c61dc90ffd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'storage/output/220321_baseline/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44bde7-8a05-43c8-8bb7-26b860442126",
   "metadata": {},
   "source": [
    "****Prepare Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7524be3-ff2a-466b-9055-02a631e847ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 666 ms, sys: 989 ms, total: 1.66 s\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "transactions = cudf.read_csv('storage/transactions_train.csv')\n",
    "articles = cudf.read_csv('storage/articles.csv')\n",
    "customers = cudf.read_csv('storage/customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2075745c-be75-4d9f-8fc2-182d90f45a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['FN'].fillna(0.,inplace=True)\n",
    "customers['Active'].fillna(0.,inplace=True)\n",
    "customers['club_member_status'].fillna('None',inplace=True)\n",
    "customers['age'] = customers['age'] / 10\n",
    "customers['age'] = customers['age'].astype(int)\n",
    "customers['fashion_news_frequency'] = customers['fashion_news_frequency'].str.lower().fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e2faaa5-0db5-4b4e-a3aa-ce00e28601cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['t_dat'] = cudf.to_datetime(transactions['t_dat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "19d082ed-181e-4b7c-a7be-1ff761e4779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_purchase_feature(df,transactions):\n",
    "    transactions['count'] = 1\n",
    "    \n",
    "    time_elapsed_last_purchase = transactions['t_dat'].max()-transactions[['customer_id','article_id','t_dat']].groupby(['customer_id','article_id'])['t_dat'].max()\n",
    "    time_elapsed_last_purchase = time_elapsed_last_purchase.dt.days\n",
    "    df = df.merge(time_elapsed_last_purchase,on=['article_id','customer_id'],how='left')\n",
    "    df = df.rename(columns={'t_dat':'time_elapsed_last_purchase'})\n",
    "    df['time_elapsed_last_purchase'].fillna(1e6,inplace=True)\n",
    "    \n",
    "    time_elapsed_first_release = transactions[['customer_id','article_id','t_dat']].groupby(['customer_id','article_id'])['t_dat'].min()-cudf.to_datetime('2018-09-01')\n",
    "    time_elapsed_first_release = time_elapsed_first_release.dt.days\n",
    "    df = df.merge(time_elapsed_first_release,on=['article_id','customer_id'],how='left')\n",
    "    df = df.rename(columns={'t_dat':'time_elapsed_first_release'})\n",
    "    df['time_elapsed_first_release'].fillna(1e6,inplace=True)\n",
    "    \n",
    "    past_purchase_prob = transactions[['customer_id','article_id','count']].groupby(['customer_id','article_id'])['count'].count().reset_index()\n",
    "    norm = transactions[['customer_id','article_id']].groupby('customer_id').count().reset_index().rename(columns={'article_id':'norm'})\n",
    "    past_purchase_prob = past_purchase_prob.merge(norm,on='customer_id')\n",
    "    past_purchase_prob['count'] = past_purchase_prob['count'] / past_purchase_prob['norm']\n",
    "    past_purchase_prob.drop(columns=['norm'],inplace=True)\n",
    "    df = df.merge(past_purchase_prob,on=['article_id','customer_id'],how='left')\n",
    "    df = df.rename(columns={'count':'past_purchase_prob'})\n",
    "    df['past_purchase_prob'].fillna(0.,inplace=True)\n",
    "    \n",
    "    total_purchase = transactions[['article_id','count']].groupby('article_id')['count'].count().reset_index().rename(columns={'count':'total_purchase'})\n",
    "    norm = transactions['count'].sum()\n",
    "    total_purchase['total_purchase'] = total_purchase['total_purchase'] / norm\n",
    "    df = df.merge(total_purchase,on='article_id',how='left')\n",
    "    df['total_purchase'].fillna(0.,inplace=True)\n",
    "    \n",
    "    number_of_purchase = transactions[['customer_id','count']].groupby('customer_id')['count'].count().reset_index().rename(columns={'count':'number_of_purchase'})\n",
    "    df = df.merge(number_of_purchase,on='customer_id',how='left')\n",
    "    df['number_of_purchase'].fillna(0.,inplace=True)\n",
    "    \n",
    "    repeated_purchase = transactions[['customer_id','article_id','count']].groupby(['customer_id','article_id'])['count'].count().reset_index().rename(columns={'count':'repeated_purchase'})\n",
    "    df = df.merge(repeated_purchase,on=['customer_id','article_id'],how='left')\n",
    "\n",
    "    return df\n",
    "    \n",
    "def article_feature_prob_vector(df,transactions,articles,article_features,postfix='_prob'):\n",
    "    transactions['count'] = 1\n",
    "    for article_feature in article_features:\n",
    "        transactions = transactions.merge(articles[['article_id',article_feature]],on='article_id',how='left')\n",
    "        norm = transactions.groupby(['customer_id'])['count'].count().reset_index()\n",
    "        norm.rename(columns={'count':'norm'},inplace=True)\n",
    "        count = transactions.groupby(['customer_id',article_feature])['count'].count().reset_index()\n",
    "        count = count.merge(norm,on='customer_id')\n",
    "        count['count'] = count['count'] / count['norm']\n",
    "        count = count.rename(columns={'count':article_feature+postfix})\n",
    "        count = count[['customer_id',article_feature,article_feature+postfix]]\n",
    "        del(norm)\n",
    "        df = df.merge(articles[['article_id',article_feature]],on='article_id',how='left')\n",
    "        df = df.merge(count,on=['customer_id',article_feature],how='left')\n",
    "    return df\n",
    "\n",
    "def customer_feature_prob_vector(df,transactions,customers,customer_features,postfix='_prob'):\n",
    "    transactions['count'] = 1\n",
    "    for customer_feature in customer_features:\n",
    "        transactions = transactions.merge(customers[['customer_id',customer_feature]],on='customer_id',how='left')\n",
    "        norm = transactions.groupby(['article_id'])['count'].count().reset_index()\n",
    "        norm.rename(columns={'count':'norm'},inplace=True)\n",
    "        count = transactions.groupby(['article_id',customer_feature])['count'].count().reset_index()\n",
    "        count = count.merge(norm,on='article_id')\n",
    "        count['count'] = count['count'] / count['norm']\n",
    "        count = count.rename(columns={'count':customer_feature+postfix})\n",
    "        count = count[['article_id',customer_feature,customer_feature+postfix]]\n",
    "        del(norm)\n",
    "        df = df.merge(customers[['customer_id',customer_feature]],on='customer_id',how='left')\n",
    "        df = df.merge(count,on=['article_id',customer_feature],how='left')\n",
    "    return df\n",
    "\n",
    "def construct_feature_df(\n",
    "        df,transactions,\n",
    "        article_features,\n",
    "        articles,\n",
    "        customer_features,\n",
    "        customers,\n",
    "        general_features=['article_id','customer_id'],\n",
    "    ):\n",
    "    df = article_feature_prob_vector(df,transactions,articles,article_features)\n",
    "    df = customer_feature_prob_vector(df,transactions,customers,customer_features)\n",
    "    df = past_purchase_feature(df,transactions)\n",
    "    df = df[\n",
    "            general_features+[f for f in df.columns if '_prob' in f] + \n",
    "            ['total_purchase','time_elapsed_last_purchase','past_purchase_prob','number_of_purchase','time_elapsed_first_release','repeated_purchase']\n",
    "        ]\n",
    "    return df\n",
    "\n",
    "def construct_candidate_dict(transactions_3w):\n",
    "    purchase_dict_3w = {}\n",
    "    for i,x in enumerate(zip(transactions_3w['customer_id'], transactions_3w['article_id'])):\n",
    "        cust_id, art_id = x\n",
    "        if cust_id not in purchase_dict_3w:\n",
    "            purchase_dict_3w[cust_id] = {}\n",
    "        if art_id not in purchase_dict_3w[cust_id]:\n",
    "            purchase_dict_3w[cust_id][art_id] = 0\n",
    "        purchase_dict_3w[cust_id][art_id] += 1\n",
    "    dummy_list_3w = list((transactions_3w['article_id'].value_counts()).index)[:12]\n",
    "    return purchase_dict_3w,dummy_list_3w\n",
    "\n",
    "def get_week(purchase_dicts,article_id):\n",
    "    for i,purchase_dict in enumerate(purchase_dicts):\n",
    "        if article_id in purchase_dict: return i\n",
    "    return 1e6\n",
    "\n",
    "def construct_candidate_df(test_df,transactions,add_random_samples=False):\n",
    "    \n",
    "    bool_1w = transactions.t_dat>transactions.t_dat.max()-pd.Timedelta(7,unit='day')\n",
    "    bool_2w = (transactions.t_dat>transactions.t_dat.max()-2*pd.Timedelta(7,unit='day'))&(transactions.t_dat<=transactions.t_dat.max()-pd.Timedelta(7,unit='day'))\n",
    "    bool_3w = (transactions.t_dat>transactions.t_dat.max()-3*pd.Timedelta(7,unit='day'))&(transactions.t_dat<=transactions.t_dat.max()-2*pd.Timedelta(7,unit='day'))\n",
    "    bool_4w = (transactions.t_dat>transactions.t_dat.max()-4*pd.Timedelta(7,unit='day'))&(transactions.t_dat<=transactions.t_dat.max()-3*pd.Timedelta(7,unit='day'))\n",
    "    bool_5w = (transactions.t_dat>transactions.t_dat.max()-5*pd.Timedelta(7,unit='day'))&(transactions.t_dat<=transactions.t_dat.max()-4*pd.Timedelta(7,unit='day'))\n",
    "    bool_6w = (transactions.t_dat>transactions.t_dat.max()-6*pd.Timedelta(7,unit='day'))&(transactions.t_dat<=transactions.t_dat.max()-5*pd.Timedelta(7,unit='day'))\n",
    "    bool_7w = (transactions.t_dat>transactions.t_dat.max()-7*pd.Timedelta(7,unit='day'))&(transactions.t_dat<=transactions.t_dat.max()-6*pd.Timedelta(7,unit='day'))\n",
    "    \n",
    "    transactions_7w = transactions[bool_7w]\n",
    "    transactions_6w = transactions[bool_6w]\n",
    "    transactions_5w = transactions[bool_5w]\n",
    "    transactions_4w = transactions[bool_4w]\n",
    "    transactions_3w = transactions[bool_3w]\n",
    "    transactions_2w = transactions[bool_2w]\n",
    "    transactions_1w = transactions[bool_1w]\n",
    "    \n",
    "    transactions_1w = transactions[bool_1w].to_pandas()\n",
    "    transactions_2w = transactions[bool_2w].to_pandas()\n",
    "    transactions_3w = transactions[bool_3w].to_pandas()\n",
    "    transactions_4w = transactions[bool_4w].to_pandas()\n",
    "    transactions_5w = transactions[bool_5w].to_pandas()\n",
    "    transactions_6w = transactions[bool_6w].to_pandas()\n",
    "    transactions_7w = transactions[bool_7w].to_pandas()\n",
    "    \n",
    "    purchase_dict_1w,dummy_list_1w = construct_candidate_dict(transactions_1w)\n",
    "    purchase_dict_2w,_ = construct_candidate_dict(transactions_2w)\n",
    "    purchase_dict_3w,_ = construct_candidate_dict(transactions_3w)\n",
    "    purchase_dict_4w,_ = construct_candidate_dict(transactions_4w)\n",
    "    purchase_dict_5w,_ = construct_candidate_dict(transactions_5w)\n",
    "    purchase_dict_6w,_ = construct_candidate_dict(transactions_6w)\n",
    "    purchase_dict_7w,_ = construct_candidate_dict(transactions_7w)\n",
    "    \n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df['customer_id'] = test_df['customer_id'].unique()\n",
    "    \n",
    "    prediction_list = []\n",
    "    week_list = []\n",
    "    \n",
    "    if add_random_samples:\n",
    "        dummy_pred = transactions['article_id'].sample(frac=1.).to_arrow().to_pylist()[:50]\n",
    "    else:\n",
    "        dummy_pred = (transactions_1w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        dummy_pred += (transactions_2w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        dummy_pred += (transactions_3w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        dummy_pred += (transactions_4w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        dummy_pred += (transactions_5w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        dummy_pred += (transactions_6w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        dummy_pred += (transactions_7w['article_id'].value_counts()).index.tolist()[:12]\n",
    "        #dummy_pred += (transactions['article_id'].value_counts()).index.to_arrow().to_pylist()[:12]\n",
    "    \n",
    "    for i, cust_id in enumerate(pred_df['customer_id']):\n",
    "        s = []\n",
    "        purchase_dicts = []\n",
    "        if cust_id in purchase_dict_1w:\n",
    "            l = sorted((purchase_dict_1w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_1w[cust_id])\n",
    "            \n",
    "        if cust_id in purchase_dict_2w:\n",
    "            l = sorted((purchase_dict_2w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_2w[cust_id])\n",
    "\n",
    "        if cust_id in purchase_dict_3w:\n",
    "            l = sorted((purchase_dict_3w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_3w[cust_id])\n",
    "\n",
    "        if cust_id in purchase_dict_4w:\n",
    "            l = sorted((purchase_dict_4w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_4w[cust_id])\n",
    "\n",
    "        if cust_id in purchase_dict_5w:\n",
    "            l = sorted((purchase_dict_5w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_5w[cust_id])\n",
    "\n",
    "        if cust_id in purchase_dict_6w:\n",
    "            l = sorted((purchase_dict_6w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_6w[cust_id])\n",
    "\n",
    "        if cust_id in purchase_dict_7w:\n",
    "            l = sorted((purchase_dict_7w[cust_id]).items(), key=lambda x: x[1], reverse=True)\n",
    "            l = [y[0] for y in l]\n",
    "            s += l\n",
    "            purchase_dicts.append(purchase_dict_7w[cust_id])\n",
    "\n",
    "        s += dummy_pred\n",
    "        \n",
    "        s = list(set(s))\n",
    "        prediction_list.append(s)\n",
    "        \n",
    "        week_list.append([get_week(purchase_dicts,aid) for aid in s])\n",
    "        \n",
    "    pred_df['article_id'] = prediction_list\n",
    "    pred_df['week'] = week_list\n",
    "    \n",
    "    return pred_df\n",
    "    \n",
    "def construct_val_df(test_df,transactions,article_features,articles,customer_features,customers,how='outer',add_random_samples=False):\n",
    "    pos_df = test_df.groupby('customer_id')['article_id'].unique().to_frame().reset_index().explode('article_id')\n",
    "    pos_df['label'] = 1\n",
    "    test_df = construct_candidate_df(test_df.to_pandas(),transactions,add_random_samples=add_random_samples).explode('article_id').reset_index(drop=True)\n",
    "    test_df = test_df.merge(pos_df.to_pandas(),on=['article_id','customer_id'],how=how)\n",
    "    test_df['label'].fillna(0,inplace=True)\n",
    "    test_df = cudf.from_pandas(test_df)\n",
    "    test_df = construct_feature_df(test_df,transactions,article_features,articles,customer_features,customers,general_features=['article_id','customer_id','label'])\n",
    "    test_df = test_df.fillna(0.)\n",
    "    test_df['article_id'] = test_df['article_id'].astype(int)\n",
    "    test_df = test_df.sort_values(['customer_id','article_id']).reset_index(drop=True)\n",
    "    return test_df\n",
    "\n",
    "def construct_test_df(test_df,transactions,article_features,articles,customer_features,customers,how='outer',add_random_samples=False):\n",
    "    test_df = construct_candidate_df(test_df.to_pandas(),transactions,add_random_samples=add_random_samples).explode(['article_id','week']).reset_index(drop=True)\n",
    "    test_df = cudf.from_pandas(test_df)\n",
    "    test_df = construct_feature_df(test_df,transactions,article_features,articles,customer_features,customers,general_features=['article_id','customer_id'])\n",
    "    test_df = test_df.fillna(0.)\n",
    "    test_df['article_id'] = test_df['article_id'].astype(int)\n",
    "    test_df = test_df.sort_values(['customer_id','article_id']).reset_index(drop=True)\n",
    "    return test_df\n",
    "\n",
    "def construct_gt_df(test_transactions):\n",
    "    gt_df = test_transactions.to_pandas().groupby('customer_id')['article_id'].agg(lambda x: x.tolist()).reset_index()\n",
    "    gt_df.columns = ['customer_id','ground_truth']\n",
    "    return gt_df\n",
    "    \n",
    "def construct_dataset(\n",
    "        transactions,\n",
    "        articles,customers,\n",
    "        trn_start_time='2020-08-31',trn_end_time='2020-09-08',\n",
    "        val_start_time='2020-09-08',val_end_time='2020-09-15',\n",
    "        test_start_time='2020-09-08',test_end_time='2020-09-15',\n",
    "        article_features=[\n",
    "            'product_group_name', 'product_type_name', \n",
    "            'graphical_appearance_name', 'perceived_colour_value_name', 'colour_group_code', \n",
    "            'index_name', 'index_group_name', \n",
    "            'section_name', 'department_name',\n",
    "        ],\n",
    "        customer_features=[\n",
    "            'FN','Active','club_member_status','age','fashion_news_frequency',\n",
    "        ],\n",
    "    ):\n",
    "    \n",
    "    trn_start_time = cudf.to_datetime(trn_start_time)\n",
    "    trn_end_time = cudf.to_datetime(trn_end_time)\n",
    "    val_start_time = cudf.to_datetime(val_start_time)\n",
    "    val_end_time = cudf.to_datetime(val_end_time)\n",
    "    test_start_time = cudf.to_datetime(test_start_time)\n",
    "    test_end_time = cudf.to_datetime(test_end_time)\n",
    "    \n",
    "    trn_transactions = transactions[(transactions.t_dat > trn_start_time) & (transactions.t_dat <= trn_end_time)]\n",
    "    val_transactions = transactions[(transactions.t_dat > val_start_time) & (transactions.t_dat <= val_end_time)]\n",
    "    test_transactions = transactions[(transactions.t_dat > test_start_time) & (transactions.t_dat <= test_end_time)]\n",
    "    gt_df = construct_gt_df(test_transactions)\n",
    "    \n",
    "    trn_df = construct_test_df(val_transactions,trn_transactions,article_features,articles,customer_features,customers,how='left')\n",
    "    pos_label = val_transactions[['article_id','customer_id']]\n",
    "    pos_label['label'] = 1\n",
    "    trn_df = trn_df.merge(pos_label,on=['article_id','customer_id'],how='left')\n",
    "    trn_df['label'].fillna(0.,inplace=True)\n",
    "    \n",
    "    trn_df = trn_df.merge(trn_df.groupby('customer_id').size().to_frame().rename(columns={0:'group_size'}),on='customer_id')\n",
    "    test_df = construct_test_df(test_transactions,val_transactions,article_features,articles,customer_features,customers,how='left')\n",
    "    \n",
    "    return trn_df.reset_index(drop=True),test_df.reset_index(drop=True),gt_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6484353c-5b9c-414a-b294-41fbe3d19784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMCollator(object):\n",
    "    def __init__(self,dfs,features,label,k=12):\n",
    "        self.dfs = dfs\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.k = k\n",
    "        \n",
    "    def x_y_group(self,data,features,target,only_x=False,verbose=False):\n",
    "        group = data.groupby('customer_id').size().to_frame('size')['size']\n",
    "        data = data.sort_values('customer_id').reset_index()\n",
    "        return data[features],data[target],group\n",
    "        \n",
    "    def get_train_dataset_by_index(self,index):\n",
    "        trn_x,trn_y,trn_grp = self.x_y_group(self.dfs[index][0],features,label)\n",
    "        return trn_x,trn_y,trn_grp\n",
    "        \n",
    "    def get_ground_truth_dataset_by_index(self,index):\n",
    "        return self.dfs[index][-1]\n",
    "    \n",
    "    def get_test_dataset_by_index(self,index):\n",
    "        return self.dfs[index][1].to_pandas()\n",
    "    \n",
    "    def construct_eval_dataset(self):\n",
    "        self.gt_df = self.get_ground_truth_dataset_by_index(0)\n",
    "        self.test_df = self.get_test_dataset_by_index(0)\n",
    "    \n",
    "    def evaluate_score(self,pred_df,gt_df,k=12,verbose=True,group_name='customer_id'):\n",
    "        from metric import mapk\n",
    "        eval_df = gt_df.merge(pred_df,on=group_name,how='left')\n",
    "        score = mapk(eval_df['ground_truth'].tolist(),eval_df['prediction'].tolist())\n",
    "        if verbose: print('map@'+str(k),score)\n",
    "        return score\n",
    "    \n",
    "    def feval(self,preds,eval_dataset):\n",
    "        pred_df = pd.DataFrame()\n",
    "        pred_df['customer_id'] = self.test_df['customer_id']\n",
    "        pred_df['article_id'] = self.test_df['article_id']\n",
    "        pred_df['prediction'] = preds\n",
    "        pred_df = pred_df.groupby('customer_id') \\\n",
    "                        .apply(lambda x: x.sort_values('prediction',ascending=False)['article_id'].tolist()[:self.k]) \\\n",
    "                        .reset_index()\n",
    "        pred_df.columns = ['customer_id','prediction']\n",
    "        score = self.evaluate_score(pred_df,self.gt_df,group_name='customer_id',verbose=False)\n",
    "        return 'MAP@'+str(self.k), score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "628e94e1-0d73-49aa-92a3-0383ba49186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_group(data,features,target,only_x=False,verbose=False):\n",
    "    group = data.groupby('customer_id').size().to_frame('size')['size']\n",
    "    data = data.sort_values('customer_id').reset_index()\n",
    "    return data[features],data[target],group\n",
    "\n",
    "def make_prediction(model,test_df,features,label,k=12,group_name='customer_id'):\n",
    "    test_x = test_df[features]\n",
    "    test_pred = model.predict(test_x)\n",
    "    test_x[group_name] = test_df[group_name]\n",
    "    test_x['article_id'] = test_df['article_id']\n",
    "    test_x['prediction'] = test_pred\n",
    "    pred_df = test_x.groupby(group_name) \\\n",
    "                    .apply(lambda x: x.sort_values('prediction',ascending=False)['article_id'].tolist()[:k]) \\\n",
    "                    .reset_index()\n",
    "    pred_df.columns = [group_name,'prediction']\n",
    "    return pred_df\n",
    "\n",
    "def evaluate_score(pred_df,gt_df,k=12,verbose=True,group_name='customer_id'):\n",
    "    from metric import mapk\n",
    "    eval_df = gt_df.merge(pred_df,on=group_name,how='left')\n",
    "    score = mapk(eval_df['ground_truth'].tolist(),eval_df['prediction'].tolist())\n",
    "    if verbose: print('map@'+str(k),score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226e2e5-ca7c-41dc-b20f-b1e4fbb5c6ac",
   "metadata": {},
   "source": [
    "****Cross validation****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e08f4-00bd-456f-9972-d5755844a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: keep_training_booster\n",
      "[LightGBM] [Warning] Unknown parameter: keep_training_booster\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.845278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4652\n",
      "[LightGBM] [Info] Number of data points in the train set: 19322727, number of used features: 20\n",
      "[LightGBM] [Warning] Unknown parameter: keep_training_booster\n",
      "[1]\tvalid_0's MAP@12: 0.019677\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's MAP@12: 0.0207268\n",
      "[3]\tvalid_0's MAP@12: 0.0210634\n",
      "[4]\tvalid_0's MAP@12: 0.0209037\n",
      "[5]\tvalid_0's MAP@12: 0.0212307\n",
      "[6]\tvalid_0's MAP@12: 0.0211498\n",
      "[7]\tvalid_0's MAP@12: 0.0214056\n",
      "[8]\tvalid_0's MAP@12: 0.021484\n",
      "[9]\tvalid_0's MAP@12: 0.0214266\n",
      "[10]\tvalid_0's MAP@12: 0.0214021\n",
      "[11]\tvalid_0's MAP@12: 0.0215277\n",
      "[12]\tvalid_0's MAP@12: 0.021463\n",
      "[13]\tvalid_0's MAP@12: 0.0216083\n",
      "[14]\tvalid_0's MAP@12: 0.0217508\n",
      "[15]\tvalid_0's MAP@12: 0.0218504\n",
      "[16]\tvalid_0's MAP@12: 0.0216948\n",
      "[17]\tvalid_0's MAP@12: 0.0214625\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label = 'label'\n",
    "scores = []\n",
    "\n",
    "for i,(t1,t2,t3,t4) in enumerate([\n",
    "        ('2020-01-01','2020-08-01','2020-09-15','2020-09-22'),\n",
    "        #('2020-01-01','2020-07-24','2020-09-07','2020-09-15'),\n",
    "        #('2020-01-01','2020-07-17','2020-09-01','2020-09-07'),\n",
    "    ]):\n",
    "    \n",
    "    trn_df,test_df,gt_df = construct_dataset(\n",
    "        transactions,\n",
    "        articles,customers,\n",
    "        trn_start_time=t1,trn_end_time=t2,\n",
    "        val_start_time=t2,val_end_time=t3,\n",
    "        test_start_time=t3,test_end_time=t4,\n",
    "    )\n",
    "    \n",
    "    features = [c for c in trn_df.columns if c not in ['article_id','customer_id','label','index','group_size']]\n",
    "    collator = LightGBMCollator([[trn_df,test_df,gt_df]],features,label)\n",
    "    trn_x,trn_y,trn_grp = collator.get_train_dataset_by_index(0)\n",
    "    collator.construct_eval_dataset()\n",
    "\n",
    "    trn_dataset = lgb.Dataset(trn_x.to_pandas(),trn_y.to_pandas(),group=trn_grp.to_pandas())\n",
    "    val_dataset = lgb.Dataset(collator.test_df[collator.features])\n",
    "    param = dict(\n",
    "        objective='lambdarank',\n",
    "        metric='map@12',\n",
    "        keep_training_booster=True,\n",
    "        early_stopping_round=5,\n",
    "        seed=0,\n",
    "        learning_rate=0.1,\n",
    "    )\n",
    "    num_round = 1000\n",
    "    bst = lgb.train(\n",
    "        param,\n",
    "        trn_dataset, \n",
    "        num_round,\n",
    "        feval=collator.feval,\n",
    "        valid_sets=[val_dataset],\n",
    "    )\n",
    "    \n",
    "    pred_df = make_prediction(bst,test_df.to_pandas(),features,label)\n",
    "    score = evaluate_score(pred_df,gt_df)\n",
    "    scores.append(score)\n",
    "print('score: ',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899a3aa-e21f-4cf7-99e2-19eae5ec8e5d",
   "metadata": {},
   "source": [
    "****Training****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe0ee52-7410-42b6-a1b6-a8a9e8997897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 6.05 s, total: 19.7 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs = []\n",
    "for i,(t1,t2,t3,t4) in enumerate([\n",
    "        ('2020-01-01','2020-08-01','2020-09-15','2020-09-22'),\n",
    "        ('2020-01-01','2020-08-01','2020-09-15','2020-09-22'),\n",
    "    ]):\n",
    "    trn_tmp,test_tmp,gt_tmp = construct_dataset(\n",
    "        transactions,\n",
    "        articles,customers,\n",
    "        trn_start_time=t1,trn_end_time=t2,\n",
    "        val_start_time=t2,val_end_time=t3,\n",
    "        test_start_time=t3,test_end_time=t4,\n",
    "    )\n",
    "    dfs.append((trn_tmp,test_tmp,gt_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebdc78e8-7c30-4f75-8986-0d8259d389b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinho.lo/.local/lib/python3.8/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: keep_training_booster\n",
      "[LightGBM] [Warning] Unknown parameter: keep_training_booster\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4718\n",
      "[LightGBM] [Info] Number of data points in the train set: 7832995, number of used features: 20\n",
      "[LightGBM] [Warning] Unknown parameter: keep_training_booster\n",
      "[1]\tvalid_0's MAP@12: 0.0202667\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's MAP@12: 0.0214225\n",
      "[3]\tvalid_0's MAP@12: 0.0214367\n",
      "[4]\tvalid_0's MAP@12: 0.0215855\n",
      "[5]\tvalid_0's MAP@12: 0.0218864\n",
      "[6]\tvalid_0's MAP@12: 0.0217541\n",
      "[7]\tvalid_0's MAP@12: 0.0214959\n",
      "[8]\tvalid_0's MAP@12: 0.0218041\n",
      "[9]\tvalid_0's MAP@12: 0.0219306\n",
      "[10]\tvalid_0's MAP@12: 0.0218384\n",
      "[11]\tvalid_0's MAP@12: 0.0221501\n",
      "[12]\tvalid_0's MAP@12: 0.0218704\n",
      "[13]\tvalid_0's MAP@12: 0.0218787\n",
      "[14]\tvalid_0's MAP@12: 0.0219306\n",
      "[15]\tvalid_0's MAP@12: 0.0219266\n",
      "[16]\tvalid_0's MAP@12: 0.0218623\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's MAP@12: 0.0221501\n",
      "CPU times: user 4min 9s, sys: 3.79 s, total: 4min 13s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = [c for c in dfs[0][0].columns if c not in ['article_id','customer_id','label','index','group_size']]\n",
    "label = 'label'\n",
    "\n",
    "collator = LightGBMCollator(dfs,features,label)\n",
    "trn_x,trn_y,trn_grp = collator.get_train_dataset_by_index(0)\n",
    "collator.construct_eval_dataset()\n",
    "\n",
    "trn_dataset = lgb.Dataset(trn_x.to_pandas(),trn_y.to_pandas(),group=trn_grp.to_pandas())\n",
    "val_dataset = lgb.Dataset(collator.test_df[collator.features])\n",
    "param = dict(\n",
    "    objective='lambdarank',\n",
    "    metric='map@12',\n",
    "    keep_training_booster=True,\n",
    "    early_stopping_round=5,\n",
    "    seed=0,\n",
    ")\n",
    "num_round = 1000\n",
    "bst = lgb.train(\n",
    "    param,\n",
    "    trn_dataset, \n",
    "    num_round,\n",
    "    feval=collator.feval,\n",
    "    valid_sets=[val_dataset],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe76b64d-65fc-45e4-b7e7-932bb089c5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'week': 0.0,\n",
       " 'product_group_name_prob': 0.0,\n",
       " 'product_type_name_prob': 2859.975051879883,\n",
       " 'graphical_appearance_name_prob': 690.2089996337891,\n",
       " 'perceived_colour_value_name_prob': 168.3333969116211,\n",
       " 'colour_group_code_prob': 774.8486862182617,\n",
       " 'index_name_prob': 2202.320114135742,\n",
       " 'index_group_name_prob': 685.1419982910156,\n",
       " 'section_name_prob': 1906.414535522461,\n",
       " 'department_name_prob': 9431.715065002441,\n",
       " 'FN_prob': 0.0,\n",
       " 'Active_prob': 562.3703994750977,\n",
       " 'club_member_status_prob': 8235.779876708984,\n",
       " 'age_prob': 4028.970016479492,\n",
       " 'fashion_news_frequency_prob': 0.0,\n",
       " 'past_purchase_prob': 12402.088165283203,\n",
       " 'total_purchase': 13010.16780090332,\n",
       " 'time_elapsed_last_purchase': 14486.576126098633,\n",
       " 'number_of_purchase': 1768.5429992675781,\n",
       " 'time_elapsed_first_release': 2697.0369720458984}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "{k:v for k,v in zip(bst.feature_name(),bst.feature_importance('gain'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acfcef-aff9-4f45-9c6e-de76393f56e1",
   "metadata": {},
   "source": [
    "****Local CV****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c269ac-e444-4de4-a565-6d1aaf6f58e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/21872152/ipykernel_193583/4083372006.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[group_name] = test_df[group_name]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 574 ms, total: 15.4 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idx = 0\n",
    "pred_df = make_prediction(bst,dfs[idx][1].to_pandas(),features,label,k=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44ea61a8-4a89-46c4-ac11-6ee333a5801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@12 0.0223621690029079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0223621690029079"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_score(\n",
    "    pred_df,\n",
    "    dfs[idx][-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24241be-bbaf-4b9a-81ab-5266f61aafab",
   "metadata": {},
   "source": [
    "****Submission****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e357f1-bdc1-4fe6-a061-9739814accfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/21872152/ipykernel_193583/4083372006.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_x[group_name] = test_df[group_name]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 8s, sys: 19.7 s, total: 5min 28s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "article_features=[\n",
    "    'product_group_name', 'product_type_name', \n",
    "    'graphical_appearance_name', 'perceived_colour_value_name', 'colour_group_code', \n",
    "    'index_name', 'index_group_name', \n",
    "    'section_name', 'department_name',\n",
    "]\n",
    "customer_features=[\n",
    "    'FN','Active','club_member_status','age','fashion_news_frequency',\n",
    "    ]\n",
    "submission_df = cudf.read_csv('storage/sample_submission.csv')\n",
    "submission_df = construct_test_df(\n",
    "    submission_df[['customer_id']],\n",
    "    transactions[(transactions.t_dat > cudf.to_datetime('2020-09-07')) & (transactions.t_dat <= cudf.to_datetime('2020-09-22'))],\n",
    "    article_features,articles,customer_features,customers,\n",
    "    how='left',\n",
    ")\n",
    "submission_df = make_prediction(bst,submission_df.to_pandas(),features,label,k=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54385cf3-69f7-4d7b-bd17-26b20b922db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['prediction'] = submission_df['prediction'].apply(lambda x: ' '.join(['0'+str(i) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f246357-a6b4-4b06-ab21-89353734fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(base_dir,exist_ok=True)\n",
    "submission_df.to_csv(os.path.join(base_dir,'submission.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6a2e80-30a0-425b-8968-1f8edf603eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>0865799006 0673677002 0850917001 0751471001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>0865799006 0751471043 0924243001 0923758001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0794321007 0918292001 0751471043 0865799006 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>0924243002 0863646001 0924243001 0923758001 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>0924243002 0863646001 0924243001 0923758001 07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371975</th>\n",
       "      <td>ffffbbf78b6eaac697a8a5dfbfd2bfa8113ee5b403e474...</td>\n",
       "      <td>0865799006 0751471043 0924243001 0923758001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371976</th>\n",
       "      <td>ffffcd5046a6143d29a04fb8c424ce494a76e5cdf4fab5...</td>\n",
       "      <td>0865799006 0751471043 0924243001 0923758001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371977</th>\n",
       "      <td>ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...</td>\n",
       "      <td>0762846027 0884081001 0673677002 0689365050 08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371978</th>\n",
       "      <td>ffffd7744cebcf3aca44ae7049d2a94b87074c3d4ffe38...</td>\n",
       "      <td>0865799006 0706016001 0850917001 0918522001 09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371979</th>\n",
       "      <td>ffffd9ac14e89946416d80e791d064701994755c3ab686...</td>\n",
       "      <td>0448509014 0673677002 0924243001 0923758001 09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1371980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               customer_id  \\\n",
       "0        00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
       "1        0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
       "2        000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
       "3        00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
       "4        00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
       "...                                                    ...   \n",
       "1371975  ffffbbf78b6eaac697a8a5dfbfd2bfa8113ee5b403e474...   \n",
       "1371976  ffffcd5046a6143d29a04fb8c424ce494a76e5cdf4fab5...   \n",
       "1371977  ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1...   \n",
       "1371978  ffffd7744cebcf3aca44ae7049d2a94b87074c3d4ffe38...   \n",
       "1371979  ffffd9ac14e89946416d80e791d064701994755c3ab686...   \n",
       "\n",
       "                                                prediction  \n",
       "0        0865799006 0673677002 0850917001 0751471001 09...  \n",
       "1        0865799006 0751471043 0924243001 0923758001 09...  \n",
       "2        0794321007 0918292001 0751471043 0865799006 07...  \n",
       "3        0924243002 0863646001 0924243001 0923758001 07...  \n",
       "4        0924243002 0863646001 0924243001 0923758001 07...  \n",
       "...                                                    ...  \n",
       "1371975  0865799006 0751471043 0924243001 0923758001 09...  \n",
       "1371976  0865799006 0751471043 0924243001 0923758001 09...  \n",
       "1371977  0762846027 0884081001 0673677002 0689365050 08...  \n",
       "1371978  0865799006 0706016001 0850917001 0918522001 09...  \n",
       "1371979  0448509014 0673677002 0924243001 0923758001 09...  \n",
       "\n",
       "[1371980 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be0895-41ec-4bc9-892e-fc74de6746bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDSai-21.12",
   "language": "python",
   "name": "rapidsai-21.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
